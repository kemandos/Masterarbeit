{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from tqdm import tqdm_notebook\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code opens the HTML file \"jobs.html\" from the given path and reads it into a BeautifulSoup object.\n",
    "\n",
    "# The \"with open\" statement ensures that the file is properly closed after the nested block of code is executed.\n",
    "\n",
    "# The \"encoding\" parameter specifies the character encoding of the file, in this case \"utf8\".\n",
    "\n",
    "# The \"soup\" variable stores the BeautifulSoup object containing the parsed HTML from the \"jobs.html\" file.\n",
    "\n",
    "\n",
    "with open(\"Master/01_Scraping/example_json_scraping_it_jobs/jobs.html\",encoding=\"utf8\") as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "['https://it-jobs.de/data-analytics-ts-manager-sm-in-frankfurt-am-main_jpr-8509e9c9d44c9fd90729beaf7c41c036/', 'https://it-jobs.de/senior-data-analyst-in-hamburg_jpr-5385db2d04c604e1767a60b590f86171/']\n"
     ]
    }
   ],
   "source": [
    "# This code defines a function called 'extracting_links_from_html' that takes one input, 'soup'.\n",
    "\n",
    "# This function will create an empty list called 'links'.\n",
    "\n",
    "# This function will then loop through every 'a' tag in 'soup'. For each 'a' tag, it will take the 'href' attribute and append it to the 'links' list.\n",
    "\n",
    "# This function will then convert the 'links' list into a set (which removes any duplicates), and then convert it back into a list.\n",
    "\n",
    "# This function will then return the list of links.\n",
    "\n",
    "# This code then calls the 'extracting_links_from_html' function with 'soup' as the input, and saves the output to a new variable called 'links'.\n",
    "\n",
    "# This code then prints the length of the 'links' list and the first two elements in the list.\n",
    "\n",
    "\n",
    "\n",
    "def extracting_links_from_html(soup):\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    links = list(set(links))\n",
    "    return links   \n",
    "links = extracting_links_from_html(soup)\n",
    "print(len(links))\n",
    "print(links[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The code is iterating through the \"links\" list and if an element in the \"links\" list is also in the \"bad_links\" list, it removes that element from the \"links\" list. \n",
    "\n",
    "# The \"final_links\" variable is then set to the modified \"links\" list. The code then prints the length of the \"final_links\" list.\n",
    "\n",
    "\n",
    "bad_links = ['https://trello.com/b/sMTo6oQp/open-source-licenses','https://it-jobs.de/', 'https://it-jobs.de/achievements/','https://it-jobs.de/join-the-fundrace/','https://it-jobs.de/arbeitgeber/','None','https://it-jobs.de/stellenangebote/unternehmen/']\n",
    "\n",
    "for element in links:\n",
    "    if element in bad_links:\n",
    "        links.remove(element)\n",
    "final_links = links\n",
    "print(len(final_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALEKSE~1\\AppData\\Local\\Temp/ipykernel_928/3928537020.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for element in tqdm_notebook(final_links):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534a65c4ee474d80955609e4053ee4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The code is importing the BeautifulSoup and tqdm modules, which are both libraries that help make web scraping easier. It is also importing the requests, json, re, datetime, and time modules. \n",
    "#\n",
    "# The requests module allows you to send HTTP requests using Python, the json module allows you to work with JSON data, the re module allows you to use regular expressions, the datetime module allows you to work with dates and times, \n",
    "# and the time module allows you to work with time-related functions.\n",
    "\n",
    "# The code is defining a function called extracting_job_postings. This function takes in an input (which in this case is a list of links to job postings) and returns a list of dictionaries. Each dictionary in the list represents a job posting, \n",
    "# and each key in the dictionary represents a piece of information about the job posting (e.g. job title, company, location, etc.).\n",
    "\n",
    "# The code is creating an empty list called data. This list will be populated with dictionaries (one dictionary for each job posting).\n",
    "\n",
    "# The code is looping through each link in the input list. For each link, the code is making an HTTP request to the link using the requests module. It is then using the BeautifulSoup module to parse the HTML of the job posting page.\n",
    "\n",
    "# The code is trying to extract the job title, company, location, job description, and post date from the job posting page. If it is unable to extract this information (e.g. because the job posting page is not in the correct format), it will set the value to NaN.\n",
    "\n",
    "# The code is creating a dictionary for each job posting with the extracted information. It is then adding this dictionary to the data list.\n",
    "\n",
    "# The code is returning the data list.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extracting_job_postings(input):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for element in tqdm_notebook(input):\n",
    "        r=requests.get(element)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        try:\n",
    "            parsed_data = json.loads(soup.find('script',type='application/ld+json').string)\n",
    "        except:\n",
    "            parsed_data = np.nan\n",
    "        regex_tags_remover = re.compile('<.*?>')\n",
    "        try:\n",
    "            jobTitle = parsed_data['title']\n",
    "        except:\n",
    "            jobTitle = np.nan\n",
    "        try:\n",
    "            Company = parsed_data['hiringOrganization']['name']\n",
    "        except:\n",
    "            Company = np.nan\n",
    "        try:\n",
    "            Location = parsed_data['jobLocation'][0]['address']['addressLocality']\n",
    "        except:\n",
    "            Location = np.nan\n",
    "        try:\n",
    "            jobDescription = re.sub(r'[\\n\\t]*','',regex_tags_remover.sub('',parsed_data['description']))\n",
    "        except:\n",
    "            jobDescription = np.nan\n",
    "        try:\n",
    "            postDate = datetime.strptime(parsed_data['datePosted'],'%Y-%m-%d').strftime('%d/%m/%Y')\n",
    "        except:\n",
    "            postDate = np.nan\n",
    "\n",
    "        data_dictionary = {\n",
    "            'jobTitle': jobTitle,\n",
    "            'Company': Company,\n",
    "            'Location': Location,\n",
    "            'jobDescription': jobDescription,\n",
    "            'postDate': postDate\n",
    "            }\n",
    "        data.append(data_dictionary)\n",
    "        time.sleep(randint(1,5))\n",
    "    return data\n",
    "\n",
    "jobs_from_it_jobs = extracting_job_postings(final_links)\n",
    "jobs_from_it_jobs_df = pd.DataFrame(jobs_from_it_jobs)\n",
    "jobs_from_it_jobs_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a csv file.\n",
    "\n",
    "jobs_from_it_jobs_df.to_csv('C:/Users/Aleksej Aikov/Desktop/Enablement/Master/01_Scraping/Scraped data/website_2/jobs_website_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a214e8e1c5ebb72ddec6f7a9c3c3b7f0b57b445dc73c48c5b4bfe8c1b1803437"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
